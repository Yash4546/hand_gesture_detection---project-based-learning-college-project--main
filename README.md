# hand__gesture__detection

Language barriers are very much still a real thing. We can 
take little steps to close that for example speech to text and 
translators have made it easier. But what about those who 
can't speak? In this project we are building an object 
detection model that allows you to translate sign language 
to text as well as speech. For interaction between normal 
people and D&M people a language barrier is created as 
sign language structure which is different from normal 
text. So they depend on vision based communication for 
interaction. If there is a common interface that converts the 
sign language to text the gestures can be easily understood 
by the other people. So research has been made for a vision 
based interface system where D&M people can enjoy
communication without really knowing each other's 
language. The aim is to develop a user friendly human 
computer interfaces (HCI) where the computer 
understands the human sign language. There are various
sign languages all over the world, namely American Sign 
Language (ASL), French Sign Language, British Sign 
Language (BSL), Indian Sign language, Japanese Sign 
Language.
